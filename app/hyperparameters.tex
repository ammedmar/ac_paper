% !TEX root = ../ac_paper.tex

\section{Hyperparameters}\label{app:hyperparameters}

Here we discuss the hyperparameters used to train the Proximal Policy Optimization (PPO) and Transformer models of \autoref{sec:rl} and \autoref{sec:lm} respectively. 
Some of the hyperparameters of PPO are given in \autoref{tab:ppo_hyperparameters} below, most of which are defined in the main text in \autoref{sec:ppo}. In addition, we note that we used the actor and critic networks with unshared parameters. Each network is a 2-layer feed forward neural network with 512 neurons and $tanh$ non-linearlities. We used Adam optimizer for training. 
\newline 

The performance of PPO is known be highly sensitive to various implementation details in addition to the choice of hyperparameters \cite{shengyi2022the37implementation, engstrom2020implementation}. We used the single-file implementation of PPO in CleanRL \cite{huang2022cleanrl} which has been well-benchmarked against the results of the original PPO paper \cite{schulman2017proximal}. Following  \cite{engstrom2020implementation}, we used advantage normalization and clipped value loss. If the KL divergence between the old and the updated policy exceeded the target KL divergence in a minibatch, we skipped the remaining minibatches in the optimization phase to avoid a large jump in policy. 
We did not ablate all of the choices of hyperparameters to understand the importance of each choice.

\begin{table}[ht]
    \centering
    \begin{tabular}{lc}
        \hline
        Hyperparameter & Value \\
        \hline
        Horizon (T) & 200 \\
        Maximum Path Length & 200 \\
        Number of parallel actors & 28 \\
        Total Number of Rollouts & $\sim 2 \times 10^5$ \\
        Maximum Learning Rate & $1.0 \times 10^{-4}$ \\
        Minimum Learning Rate & 0 \\
        Learning Rate Schedule & Linear Decay \\
        Number of epochs & 1 \\
        Number of minibatches & 4 \\
        Optimization minibatch size & 1400 \\
        Discount ($\gamma$) & 0.999 \\
        GAE parameter ($\lambda$) & 0.95 \\
        Clipping parameter $\epsilon$ & 0.2 \\
        Value Loss coefficient, $c_1$ & 0.5 \\
        Entropy Loss coefficient, $c_2$ & 0.01 \\
        Adam epsilon parameter & $10^{-5}$ \\
        Target KL divergence & 0.01 \\
        \hline
    \end{tabular}
    \caption{Table of Hyperparameters}
    \label{tab:ppo_hyperparameters}
\end{table}

\fixme{Continue from here.}
We used an 8-layer transformer model with the embedding space dimension of $512$ and 4 attention heads. The context window of the Transformer had length 1024. We used a batch size of $12$ and constant learning rate of $6 \times 10^{-5}$. We trained for a total of 25000 iterations. We used Adam hyperparameters $\beta_1 = 0.9$, $\beta_2 = 0.99$ and a dropout value of $0$. Cite nanoGPT I think.