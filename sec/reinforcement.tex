% !TEX root = ../ac_paper.tex

\section{Reinforcement Learning}

While greedy search algorithm performs better than breadth first search, it has some of the same downsides. Namely, it is memory inefficient, and we cannot leverage the parallelizability of modern hardware architectures. A simple candidate for algorithms that do not have these downsides are reinforcement learning algorithms. In particular, the policy gradient algorithms, which we will review in section 4.1 are memory efficient and can be trained in a highly distributed manner. We will note that despite the use of only a fraction of compute generally available for research purposes, policy gradient algorithms seem to do well. In particular, they perform better than breadth first search algorithm in terms of the number of presentations of the Miller-Schupp series that they are able to solve, and they are able to give shorter sequences of AC moves compared to the greedy search algorithm in cases where they solve a presentation. 

This section is divided as follows: in subsection 4.1, we discuss how the problem underlying Andrews-Curtis conjecture can be modelled as a Markov Decision Process. In subsection 4.2, we will discuss some details of a specific reinforcement learning algorithm, called Proximal Policy Optimization algorithm that we used to find sequences of AC moves. Finally, in subsection 4.3, we discuss the results of our work, comparing the performance of PPO with that of the classical search algorithms studied in the previous section. 

\subsection{Markov Decision Process}

A Markov Decision Process is a 5-tuple $(S, A, R, P, \rho)$ where 
\begin{itemize}
	\item $S$ is the space of states, 
	\item $A$ is the set of actions, i.e. $a \colon S \to S \ \forall \ a \in A$, 
	\item $R \colon S \times A \times S \to \mathbb{R}$ is the ``reward" function, 
	\item $P \colon S \times A \to \mathcal{P}(S)$ is the transition probability function, and 
	\item $\rho$ is the initial probability distribution of states. 
\end{itemize}

The schematic picture of how these objects interact with each other is as follows. We start with a state $s_0$ sampled from the distribution $\rho$ and take an action $a_0$. This results in a state $s_1$ with probability $P(s_1 \mid s_0, a_0) $. The transition gets a ``reward" $r_0 = R(s_0, a_0, s_1)$ which quantifies the effectiveness of the action in contributing toward achieving an ultimate goal. From state $s_1$, we repeat this process, obtaining a trajectory of states
\[
\tau = \left( s_0, a_0, s_1, a_1, \cdots \right)
\]
The goal of this process is to maximize the cumulative return,
\[
R(\tau) = \sum\limits_{t=0}^{T} \gamma^t R(s_t, a_t, s_{t+1})
\]
Here, $T$ is the length of the trajectory and $\gamma \in \left(0, 1 \right)$ is the ``discount factor" that assigns smaller weight to the reward values obtained farther in the future. 
\newline

For a given problem at hand, we might not \textit{a priori} know the actions $\{a_t\}$ and states $\{s_{t+1}\}$ that maximize the return. Deep reinforcement learning presents a solution to this problem: we train a neural network that learns a map from states to actions with the objective of maximizing the cumulative return $R(\tau)$. More precisely, we learn a map called the ``policy" function $\pi : S \to \mathcal{P}(A)$ and sample actions from it at each time step, $a_t \sim \pi(\cdot \mid s_t)$. The specific neural network architecture and the objective function we used in this paper is reviewed in \autoref{sec:ppo}.
\newline 

In the case of the problem under investigation in this paper, i.e. finding a sequence of AC transformations that trivialize a balanced presentations, $S$ is the set of all balanced presentations of the trivial group with a fixed number of generators, and $A$ is the set of AC transformations. We need to choose a suitable reward function that assigns a larger value to the presentations that are ``closer" to the trivial state. We set it equal to the negative of the total length of the resulting presentation. A trajectory is terminated when we reach a state with reward $-2$.
\footnote{Our choice of rewards depends only on the resulting state $s_{t+1}$ and not on the initial state $s_t$ or the action $a_t$.}
\newline



\subsection{Proximal Policy Optimization} \label{sec:ppo}

The goal of our optimization process is to find a policy that maximizes the cumulative return. The most naive way to achieve this goal is through an algorithm known as the ``vanilla policy gradient" algiorithm. We perform gradient updates guided by the expected return $J(\pi_\theta) \equiv \mathbb{E}_{\tau \sim \pi_\theta} R(\tau)$, where the expectation is over a set of trajectories consisting of states and actions sampled according to our current policy, 
\[
\theta_{k+1} = \theta_k + \nabla_\theta J(\pi_\theta)
\]

One can show that this update depends on the gradient of the logarithm of the policy function itself and an ``advantage function" \( A^\pi (s, a) \) which quantifies the relative benefit of taking action \( a \) in state \( s \) under policy \( \pi \).
\footnote{
Mathematically, the advantage function is the difference between ``on-policy action-value function" $Q^{\pi}(s, a) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau) \mid s_0 = s, a_0 = a]$ and the ``on-policy value function, $V^{\pi}(s) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau) \mid s_0 = s]$." These functions give the expected cumulative return if we start with a fixed initial state (as well as the action in the case of the action-value function).
}
\begin{align*}
	\nabla_\theta J(\pi_\theta) &= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum\limits_{t=0}^T \nabla_\theta \log 								\pi_\theta (a_t \mid s_t) A^{\pi_\theta} (s_t, a_t) \right]
\end{align*}
This is equivalent to optimizing the objective function, 
\[
L^{PG} = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum\limits_{t=0}^T \log 								\pi_\theta (a_t \mid s_t) A^{\pi_\theta} (s_t, a_t) \right].
\]
Though simple, this algorithm has the downside of potentially taking the updated policy too far from the current policy.

Proximal Policy Optimization (PPO) algorithms seeks to make these updates more robust by limiting the extent to which the policy \( \pi \) can change in a single update. This is crucial to avoid destructive large updates that can destabilize the learning process. PPO implements this through a novel objective function that includes a clipped probability ratio between the new policy \( \pi_\theta \) and the old policy \( \pi_{\text{old}} \), thus constraining the updates within a predefined range. The clipped objective function is given by:
\[
L^{CLIP}(\theta) = \mathbb{E}_t\left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t) \right]
\]
where \( r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)} \) represents the probability ratio, and \( \epsilon \) is a small positive constant (commonly set around 0.1 or 0.2).

The dual aspects of this function — the raw and the clipped ratio — serve to refine the policy iteratively in a way that curtails excessively large policy updates, maintaining the essential stability of the learning progression. This modification allows PPO to enjoy the benefits of larger updates when they are safe, while avoiding significant performance regressions, which are common pitfalls in earlier policy gradient methods. \fixme{Simplify this paragraph.}



\footnote{cite actor-critic}

Include a comment about how it's distributed/parallel.

\subsection{Results}

\begin{enumerate}
\item Shorter sequences of AC moves. 
\end{enumerate}
