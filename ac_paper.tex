%%%%%%%%%%%%%%%%%%%%%%
\documentclass{amsart}
\input{aux/style}
\input{aux/usualcmds}

%%%%%%%%%%%%%%%%%%%%%%
\input{aux/packages} % add packages here
\input{aux/commands} % add commands here
\addbibresource{aux/bibliography.bib} % add references here

%%%%%%%%%%%%%%%%%%%%%%
\title[What makes math problems hard for RL]{What makes math problems hard for reinforcement learning: a case study}
% \input{sec/authors}

\date{\today}
\subjclass[2020]{68T09, 62R07, 55N31, 62R40}
\keywords{Reinforcement learning, Andrews--Curtis conjecture, automated reasoning, search algorithms, topological data analysis, large language models, unknot diagrams}

\begin{document}
	\vspace*{1cm}
	\begin{center}
		\textsc{---PREPRINT---}
	\end{center}
	\vspace*{1cm}

	\input{sec/abstract}
	\maketitle
	\tableofcontents
	\input{sec/introduction}
	\input{sec/acknowledgment}
	\input{sec/conjecture}
	\input{sec/search}
	\input{sec/reinforcement}
	\input{sec/algo}
	\input{sec/language}
	\input{sec/barcodes}
	\input{sec/stable}
	\appendix
	\input{app/hyperparameters}
        \todo{Add details on choices of hyperparameters in the experiments for scaling trends of Section 5.}
        \todo{Comment that normalizing rewards through a running estimate instead of rescaling rewards helps improve performance. Also mention the order of clipping and normalizing rewards. In the equation in section 4.3, we clip rewards, but it helps to normalize first and then clip.}
        \input{app/RLpath381}
	\input{app/neighborhoods}
	\input{app/algorithm}
	\input{sec/funding}
	\sloppy
	\printbibliography
    \input{sec/legacy}
    \todos
\end{document}